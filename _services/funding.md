---
title: "Research Funding"
date: 2018-12-28T15:14:39+10:00
weight: 4
---

List of sponsored activities and opportunities

## High Energy Physics Research on the ATLAS Experiment at the Large Hadron Collider {#ATLAS}

![DOE]({{"/images/doe.png" | relative_url }}){: width="150" }

The UMass ATLAS comprises five faculty (Ben Brau, Rafael Coelho Lopes de Sá, Carlo Dallapiccola, Verena Martinez Outschoorn, and Stéphane Willocq). The focus area of the group is exploitation of the large Run 2 dataset collected at $$\sqrt{s} = 13\,\text{TeV}$$ and the development of new analyses with the growing Run 3 dataset being collected at $$\sqrt{s} = 13.6\,\text{TeV}$$. Our activities include physics analysis in the search for physics beyond the Standard Model and the measurement of Higgs boson properties. We search for new phenomena with direct searches for Higgs boson decays into exotic new particles that are either prompt or long lived. We also measure Higgs boson production in promising topologies or phase space regions (e.g. at high-$$p_T$$ , in the off-shell regime) where BSM effects may be more prominent.

A second focus area is on hardware upgrade activities, computing, and software development for Run 3 and Run 4 at the HL-LHC. We work on hardware development for the ITk Pixel Inner System as well as on the operation of the muon trigger processor for the NSW and the development of the L0MDT for the HL-LHC. On the software side, we work on tracking-related developments, including the implementation of the ACTS modern tool set for muon reconstruction planned for Run 4 at the HL-LHC, and support of the muon spectrometer reconstruction for the current LHC operations.

More information: [UMass ATLAS group](http://blogs.umass.edu/eppex)

### Opportunities

We are looking for graduate students and postdoctoral researchers interested in joining our group. Contact me to learn about current availability.

## TAC-HEP: Training to Advance Computational HEP in the Exascale Era {#TACHEP}

![DOE]({{"/images/doe.png" | relative_url }}){: width="150" }

The complexity, computational needs and data volumes of current and future high energy physics (HEP) experiments are increasing dramatically. We are quickly approaching the Exascale era. This challenge, coupled with the rapidly evolving computing architectures and storage technologies, requires a paradigm shift in software and computing to ensure effective and efficient use of resources in the quest to address the science goals of the HEP experiments. The curriculum of a typical graduate school program has, however, not evolved at the same rate. The “on the job” experience of HEP students is often insufficient for training the next generation of software and computing experts. We propose a new traineeship program in computational HEP to bridge this gap, including relevant targeted coursework, development of specialized training modules and a structured R&D experience integrated with the training to advance workforce development for the future. This project brings together three universities (University of Wisconsin-Madison, the University of Massachusetts-Amherst, and Princeton University) and two national labs (Fermi National Accelerator Laboratory and Brookhaven National Laboratory) on three scientific missions (LHC, DUNE, and the Vera Rubin Observatory), across three frontiers (Energy, Intensity, and Cosmic frontiers) to maximize the impact on computational HEP.

### Opportunities

We are looking for graduate student fellows for this program. A fellow in the TAC-HEP program will receive a graduate level-education in both high-energy physics and computer science. The fellowship provides full funding and stipend during the program. The fellowship program includes:

- **Physics graduate courses**: Quantum Mechanics, Electrodynamics, Statistical Mechanics, Mathematical Methods, Classical Mechanics, ...
- **Computer Science graduate courses**: Computer Algorithms, Algorithms for Data Science, Machine Learning, ...
- **Training Modules**: Programing in Heterogeneous Architecture (FPGA and GPU), Distributed/Scaling Computing, Data Analysis Facilities, ...

## AccelNet HSF-India {#HSFindia}

![NSF]({{"/images/nsf.svg" | relative_url }}){: width="150" }

HSF-India will join networks in India to networks in the U.S. and Europe in order to build the international research software collaborations required to reach the science goals of experimental particle, nuclear and astroparticle physics experiments. Unanswered questions within reach of scientists include the origin of dark matter, properties of the Higgs Boson, the structural makeup of matter, and the mechanism to explain neutrino mass. These are some of the highest priority science drivers to be addressed by next-generation experimental facilities including the high-luminosity Large Hadron Collider at CERN, the Deep Underground Neutrino Experiment at Fermilab, and the Electron Ion Collider at Brookhaven National Laboratory. To fully realize their discovery potential a new generation of software algorithms and approaches is required. Software is an intellectual product of this research, not just a critical tool. It has become a critical element to design and maximize the physics discovery potential of large data intensive science projects. Building these research software collaborations is challenging and inherently international matching the international nature of the experimental undertakings themselves. HSF-India will provide U.S. students, postdocs and early career personnel significant experience in international team science through engagement in a diverse research community.

![HSF India Institutions]({{"/images/HSFindia_collaborators.png" | relative_url }}){: width="300" }

More information [https://research-software-collaborations.org/](https://research-software-collaborations.org/)

### Opportunities

We are looking for software and computing projects in HEP that could be developed in collaboration with institutions in India.

## ITk Pixel Inner System Local Supports

![DOE]({{"/images/doe.png" | relative_url }}){: width="150" }

The two innermost layers of the ITk Pixel Detector is called *ITk Pixel Inner System* and is being built in the US. US deliverables include electrical services, such as cables and optical data transmission; local supports of the detector and stave construction; modules, including assembly, testing, and front-end chip development; off-detector electronics aimed at power supply and distribution; and support for such activities as global test stands, test beams, and radiation testing. We work on the design and fabrication of *ITk Pixel Inner System Local Supports*, which are low-mass, highly-rigid structures composed of carbon-fiber laminate platforms joined by thermally-conducting graphite foam and a stiff vertical brace, with integrated titanium cooling tubes.  The local supports provide mechanical support and thermal control for the sensor modules and services in the ITk Pixel Inner System.

## Northeast Tier 2

![NSF]({{"/images/nsf.svg" | relative_url }}){: width="150" }

The bulk of computing resources available to US physicists working on the ATLAS experiment at the Large Hadron Collider is found at a *Tier 1* center at Brookhaven National Laboratory and at four *Tier 2* centers spread around the  United States.  One of these four is called the **Northeast Tier 2** Center (NET2). NET2 is located at the [Massachusetts Green High Performance Computing Center](https://www.mghpcc.org/) and is operated as a collaboration between UMass and Harvard University.  NET2 also shares its resources via the Open Science Grid so that projects like LIGO  can compute on NET2 during times of off-peak demand.

![MGHPCC]({{"/images/mghpcc.jpg" | relative_url }}){: width=300 }

## FPGA-based NN Architectures and Autonomous Learning in Trigger Systems

![DOE]({{"/images/doe.png" | relative_url }}){: width="150" }

We develop new Machine Learning (ML) algorithms implemented in programmable electronic FPGA devices for use in real time systems, such as the muon trigger system of the ATLAS detector in future runs of the Large Hadron Collider (LHC). The physics program at the LHC depends on the ability to identify signatures of interest from a deluge of proton-proton interactions by using high-speed high-bandwidth electronics relying on FPGAs. ML and Artificial
Intelligence (AI) methods running on the FPGAs may improve the performance of such systems and, particularly in the case of AI methods, reduce the engineering needs for the optimization and maintenance of such systems in the long term. AI solutions will allow the FPGA performing trigger decisions to learn from the data collected and update the ML algorithms to counteract effects that reduce the accuracy of the algorithms and data-taking efficiency.
